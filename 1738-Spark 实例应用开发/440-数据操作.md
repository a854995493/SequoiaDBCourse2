---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍 Spark 引擎解析 SQL 的工作机制，在实验中将通过 JDBC 在 Spark SQL 中创建 SequoiaDB 集合的映射表，通过 JDBC 提交查询、插入和批量插入 SQL 语句操作 SequoiaDB 集合的数据。

#### 请点击右侧选择使用的实验环境

#### 部署架构

本课程中 SequoiaDB 巨杉数据库的集群拓扑结构为三分区单副本，其中包括：

* 1 个 SequoiaSQL-MySQL 数据库实例节点
* 1 个引擎协调节点
* 1 个编目节点
* 3 个数据节点

![1738-410-06](https://doc.shiyanlou.com/courses/1738/1207281/ff2754d609aba12340efeb27ce0645bb-0)

在当前实验的部署架构中，Spark 通过 MySQL 实例间接访问 SequoiaDB 存储集群，也可以通过 SequoiaDB 的 Spark 连接驱动直接访问底层的 SequoiaDB 存储集群。

详细了解 SequoiaDB 巨杉数据库系统架构：

* [SequoiaDB 系统架构](http://doc.sequoiadb.com/cn/sequoiadb-cat_id-1519649201-edition_id-0)

#### 实验环境

课程使用的系统环境为 Ubuntu 16.04.6 LTS 版本。SequoiaDB 数据库引擎以及 SequoiaSQL-MySQL 实例均为 3.4 版本。Spark 计算引擎为 3.4 版本。

## 打开项目

#### 打开 IDEA

1）打开 IDEA 代码开发工具。

![1738-410-07](https://doc.shiyanlou.com/courses/1738/1207281/72397a857808ab74f01b042f07ea0a27-0)

#### 打开 SCDD-Spark 项目

2）打开 Spark 实验的项目，在该项目中完成所有实验步骤。

![1738-410-08](https://doc.shiyanlou.com/courses/1738/1207281/5497d93bf19ee0442b4ae79b4cd8d39a-0)

#### Maven 依赖

实验环境中已经在 Maven 本地仓库添加了实验所需的依赖。当前实验使用到的 jar 包依赖以及依赖说明如下：

* hive-jdbc-1.2.1.spark.jar

  用于在 Java 程序中通过 JDBC 访问 Spark SQL

* spark-sequoiadb_2.11-3.2.4.jar

  用于在 Spark SQL 中创建 SequoiaDB 集合的映射

* mysql-connector-java-5.1.47.jar

  用于在 Java 程序中通过 JDBC 访问 MySQL 实例

* ibatis2-common-2.1.7.597.jar

  调用 ScriptRunner 执行 sql 文件写入 MySQL 实例，以此初始化 SequoiaDB 集合中的数据

pom.xml 文件位置：

![1738-410-pom文件位置](https://doc.shiyanlou.com/courses/1738/1207281/822fa966b397b80c9eabbf0472eb52c4-0)

当前实验中使用到的 Maven 依赖如下：

![1738-410-maven1](https://doc.shiyanlou.com/courses/1738/1207281/b0f437f18a0a276bbf404da17fef9a2c-0)

![1738-410-maven2](https://doc.shiyanlou.com/courses/1738/1207281/3d62b4c63338baa02ce4337f029ff166-0)

![1738-420-maven3](https://doc.shiyanlou.com/courses/1738/1207281/fd8e22c3304df6f328663a8b30b0c423-0)

![1738-420-maven4](https://doc.shiyanlou.com/courses/1738/1207281/e262833f73a5b167d89ba087e930642d-0)

> **说明**
>
> spark-sequoiadb_2.11-3.2.4.jar 可以在 [SequoiaDB 巨杉数据库下载中心](http://download.sequoiadb.com/cn/driver) 获取。

#### 打开当前实验的 Package

3）如图所示找到当前实验程序所在 Package，在该 Package 中完成后续实验步骤：

![1738-440-package](https://doc.shiyanlou.com/courses/1738/1207281/0c9888fba451f779404357c222cac6ae-0)

## 初始化 SequoiaDB 集合数据

#### 概述

调用已定义的类和方法，通过 JDBC 在 MySQL 实例运行 SQL 脚本创建后续实验步骤中使用到的 department 表及数据，由于 MySQL 实例默认使用 sequoiadb 引擎，在 MySQL 实例中建表的同时会自动在 SequoiaDB 中创建对应的集合。

#### 操作步骤

1）右键点击 DataOperationMainTest 类，选择 Create/Edit DataOperationMainTest.main() 编辑主函数参数。

![1738-440-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/f6889e76389f71ce2371615a2ad45aee-0)

5）编辑主函数参数为 showsource。

![1738-440-参数0](https://doc.shiyanlou.com/courses/1738/1207281/f2a90bebc1858a8b2fdf916f043fff55-0)

6）右键点击 DataOperationMainTest 类，选择 Run DataOperationMainTest .main() 运行程序。

![1738-440-运行1](https://doc.shiyanlou.com/courses/1738/1207281/54add0595ab5d3bb7caae53400ba9771-0)

7）查看运行结果。

![1738-440-结果0](https://doc.shiyanlou.com/courses/1738/1207281/4960fb2e9da7215da404096e25ca8d33-0)

## 映射 SequoiaDB 集合

#### 概述

在 Spark SQL 中创建 SequoiaDB 集合的映射表，在应用中就可以通过 JDBC 提交映射表的 DML 语句来达到操作 SequoiaDB 集合数据的目的。程序会自动初始化 SequoiaDB 集合的数据，在此只需要创建映射表并查看实验初始的数据内容。

#### 操作步骤

1）打开 InitTable 类。

![1738-440-打开InitTable](https://doc.shiyanlou.com/courses/1738/1207281/91ecc8644fb580102c97982b00436371-0)

2）复制创建映射表程序代码。程序中将调用封装好的类获取 JDBC 连接，使用 JDBC 连接创建 Statement 执行创建映射表 SQL 语句，建表完成后将查询映射表的数据内容，并将查询结果集打印至控制台。所有数据操作执行完毕后将释放 JDBC 资源。

```java
// Get Hive JDBC connection
Connection connection = HiveUtil.getConnection();
// Initialize Statement
Statement statement = null;
// Initialize ResultSet
ResultSet resultSet = null;
try {
    // Create Statement
    statement=connection.createStatement();
    // Drop the existing department table
    String dropDepartment = "DROP TABLE if EXISTS department";
    // Execute the SQL statement of drop table
    statement.execute(dropDepartment);
    // Create department mapping table
    String linkDepartment =
            "CREATE TABLE department " +
                    "using com.sequoiadb.spark  " +
                    "options( " +
                    "host 'sdbserver1:11810', " +
                    "collectionspace 'sample', " +
                    "collection 'department' " +
                    ")";
    // Execute the SQL statement of create mapping table
    statement.execute(linkDepartment);
    // Get the structure of mapping table
    String showDesc = "desc department";
    // Execute the SQL statement of getting mapping table structure to get the result set
    resultSet=statement.executeQuery(showDesc);
    // Call the predefined result set to beautify the utility class and print the result set
    System.out.println("Printing table structure...");
    ResultFormat.printResultSet(resultSet);
    // Query the records of mapping table
    String queryResultSet = "SELECT * FROM department";
    // Execute the SQL statement of query mapping table record to get result set
    resultSet=statement.executeQuery(queryResultSet);
    // Call the predefined result set to beautify the utility class and print the result set
    System.out.println("Printing result set...");
    ResultFormat.printResultSet(resultSet);
} catch (SQLException e) {
    e.printStackTrace();
}finally {
    // Release Hive JDBC source
    HiveUtil.releaseSource(resultSet,statement,connection);
}
```

> **说明**
>
> ResultFormat 工具类为已经封装好的查询结果集打印类，用于美化结果集的打印；HiveUtil 工具类为已经封装好的 Hive JDBC 工具类，用于获取 JDBC 连接以及释放 JDBC 资源。封装的工具类在此不做赘述。

3）将创建数据库代码粘贴至 InitTable 类 mappingTable 方法的 TODO code 1 注释区间内。

![1738-440-TODO1](https://doc.shiyanlou.com/courses/1738/1207281/3d00020ac0dae8b6fc33111f18a2f907-0)

代码块粘贴后如图所示（为展示效果已将 try 代码块折叠）：

![1738-440-DONE1](https://doc.shiyanlou.com/courses/1738/1207281/e551f419b9a04fde8fe03e82eed992a0-0)

4）右键点击 DataOperationMainTest 类，选择 Create/Edit DataOperationMainTest.main() 编辑主函数参数。

![1738-440-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/f6889e76389f71ce2371615a2ad45aee-0)

5）编辑主函数参数为 mappingtable。

![1738-440-参数1](https://doc.shiyanlou.com/courses/1738/1207281/ffa6cee5fb17e66fb0135badaa720086-0)

6）右键点击 DataOperationMainTest 类，选择 Run DataOperationMainTest .main() 运行程序。

![1738-440-运行1](https://doc.shiyanlou.com/courses/1738/1207281/54add0595ab5d3bb7caae53400ba9771-0)

7）查看运行结果。

![1738-440-结果1](https://doc.shiyanlou.com/courses/1738/1207281/6a7f140fcc3be5b03acb20bd9e99b81e-0)

## 查询表中记录

#### 概述

Spark SQL 在处理 SQL 语句时的工作机制如下图所示：

![1738-440-01](https://doc.shiyanlou.com/courses/1738/1207281/036f44e333e7d57a85e5327247f53dd6-0)

Spark SQL 工作机制的核心为 **Catalyst Optimizer** ，它将用户程序中的 SQL/Dataset/DataFrame 经过一系列操作，最终转化为 Spark 系统中执行的 RDD（弹性分布式数据集）。上图 Catalyst Optimizer 的解析流程解读如下：

* 将 SQL 或 DataFrame 经过词法和语法解析生成未绑定的逻辑计划，之后会使用不同的 Rule 应用到该逻辑计划上

  > **说明**
  >
  > 由于 Catalyst 的 parser 不支持 update 和 delete，因此实验中将只对 Spark SQL 的 select 和 insert 操作进行演示。在实际应用场景中，往往将经过聚合、计算等处理后的 Spark SQL 结果集保存到新表中，查询使用新表，以此来满足更新表内数据的需求。

* Analyzer 通过一定的规则和数据元数据（SessionCatalog、Hive MetaStore）解析未绑定的逻辑计划，生成绑定的逻辑计划

* Optimizer 将绑定的逻辑计划经过一系列的合并、列裁剪和过滤器下推等操作后生成优化的逻辑计划

* Planner 依照 Planning Strategies 对优化的逻辑计划进行 Transform 生成物理可执行计划。每个 Strategy 将某个逻辑算子转化成对应的物理算子，最终会变成 RDD 的具体操作。

* 执行物理计划计算 RDD

当前实验步骤将通过 JDBC 提交映射表的查询语句，并将查询结果集打印到控制台。本步骤中定义的查询并打印结果集的方法将在后续的数据操作实验步骤中调用，以便观察数据操作后表中的记录变化。

#### 操作步骤

1）打开 InitTable 类。

![1738-440-打开InitTable](https://doc.shiyanlou.com/courses/1738/1207281/91ecc8644fb580102c97982b00436371-0)

2）复制查询记录代码。程序中将调用封装好的类获取 JDBC 连接，使用 JDBC 连接创建 Statement 执行查询 SQL 语句，并将查询结果集打印至控制台。所有数据操作执行完毕后将释放 JDBC 资源。

```java
// Get Hive JDBC connection
Connection connection = HiveUtil.getConnection();
// Initialize Statement
Statement statement = null;
// Initialize ResultSet
ResultSet resultSet = null;
try {
    // Create Statement
    statement = connection.createStatement();
    // Query the records of mapping table
    String queryResultSet = "SELECT * FROM " + tableName;
    // Execute the SQL statement of query mapping table record to get result set
    resultSet = statement.executeQuery(queryResultSet);
    // Call the predefined result set to beautify the utility class and print the result set
    ResultFormat.printResultSet(resultSet);
} catch (SQLException e) {
    e.printStackTrace();
} finally {
    // Release Hive JDBC source
    HiveUtil.releaseSource(resultSet, statement, connection);
}
```

> **说明**
>
> ResultFormat 工具类为已经封装好的查询结果集打印类，用于美化结果集的打印；HiveUtil 工具类为已经封装好的 Hive JDBC 工具类，用于获取 JDBC 连接以及释放 JDBC 资源。封装的工具类在此不做赘述。

3）将查询记录代码粘贴至 DataOperation 类 getAll 方法的 TODO code 2 注释区间内。

![1738-440-TODO2](https://doc.shiyanlou.com/courses/1738/1207281/9d8c9e362170511fec4624b0086a07c7-0)

代码块粘贴完毕后，完整代码如图所示：

![1738-440-DONE2](https://doc.shiyanlou.com/courses/1738/1207281/b740060584d6cf87a54ea0e2378b93f6-0)

4）右键点击 DataOperationMainTest 类，选择 Create/Edit DataOperationMainTest.main() 编辑主函数参数。

![1738-440-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/f6889e76389f71ce2371615a2ad45aee-0)

5）编辑主函数参数为 showrecords。

![1738-440-参数2](https://doc.shiyanlou.com/courses/1738/1207281/a5f727299655de3ffb2beb0ccf49af8f-0)

6）右键点击 DataOperationMainTest 类，选择 Run DataOperationMainTest .main() 运行程序。

![1738-440-运行1](https://doc.shiyanlou.com/courses/1738/1207281/54add0595ab5d3bb7caae53400ba9771-0)

7）查看运行结果。

![1738-440-结果2](https://doc.shiyanlou.com/courses/1738/1207281/572d9247b3a761cb2037da225957641b-0)

## 插入单条记录

#### 概述

当前实验步骤将通过 JDBC 提交向映射表插入单条记录语句，并调用已定义好的查询方法将查询结果集打印到控制台。

#### 操作步骤

1）打开 InitTable 类。

![1738-440-打开InitTable](https://doc.shiyanlou.com/courses/1738/1207281/91ecc8644fb580102c97982b00436371-0)

2）复制插入单条记录代码。程序中将调用封装好的类获取 JDBC 连接，使用 JDBC 连接创建 Statement 执行插入记录 SQL 语句，并调用前文中定义的查询方法将查询结果集打印至控制台展示插入记录结果。所有数据操作执行完毕后将释放 JDBC 资源。

```java
// Get Hive JDBC connection
Connection connection = HiveUtil.getConnection();
// Initialize Statement
Statement statement = null;
try {
    // Create Statement
    statement = connection.createStatement();
    // Insert a single record
    String insert = "INSERT INTO department VALUES (10,'Business Department')";
    // Execute the SQL statement of insert a single record
    statement.executeUpdate(insert);
    // Call the query method to print the current department table result set
    getAll("department");
} catch (SQLException e) {
    e.printStackTrace();
} finally {
    // Release Hive JDBC source
    HiveUtil.releaseSource(null, statement, connection);
}
```

> **说明**
>
> ResultFormat 工具类为已经封装好的查询结果集打印类，用于美化结果集的打印；HiveUtil 工具类为已经封装好的 Hive JDBC 工具类，用于获取 JDBC 连接以及释放 JDBC 资源。封装的工具类在此不做赘述。

3）将查询记录代码粘贴至 DataOperation 类 insert 方法的 TODO code 3 注释区间内。

![1738-440-TODO3](https://doc.shiyanlou.com/courses/1738/1207281/f8b753414a882b07bb5fcf973e9f591b-0)

代码块粘贴完毕后，完整代码如图所示：

![1738-440-DONE3](https://doc.shiyanlou.com/courses/1738/1207281/2633bd9ede0ff5df194774a4ee3b0147-0)

4）右键点击 DataOperationMainTest 类，选择 Create/Edit DataOperationMainTest.main() 编辑主函数参数。

![1738-440-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/f6889e76389f71ce2371615a2ad45aee-0)

5）编辑主函数参数为 insert。

![1738-440-参数3](https://doc.shiyanlou.com/courses/1738/1207281/1431a78db2adcd8885ff86dcea4a6ce3-0)

6）右键点击 DataOperationMainTest 类，选择 Run DataOperationMainTest .main() 运行程序。

![1738-440-运行1](https://doc.shiyanlou.com/courses/1738/1207281/54add0595ab5d3bb7caae53400ba9771-0)

7）查看运行结果。

![1738-440-结果3](https://doc.shiyanlou.com/courses/1738/1207281/47a62c1b9d309b4b8d3934c0365c498f-0)

可以在查询结果集中看到插入的记录。

## 批量插入记录

#### 概述

#### 操作步骤

1）打开 InitTable 类。

![1738-440-打开InitTable](https://doc.shiyanlou.com/courses/1738/1207281/91ecc8644fb580102c97982b00436371-0)

2）复制批量插入记录代码。程序会调用封装好的类初始化批量插入记录的目标集合 new_department。程序中将调用封装好的类获取 JDBC 连接，使用 JDBC 连接创建 Statement，通过 Statement 先执行创建映射表 SQL 语句，之后通过 Statement 执行批量插入记录 SQL 语句，将 department 表中数据插入到 new_department 表中，并调用前文中定义的查询方法将查询结果集打印至控制台展示批量插入结果。所有数据操作执行完毕后将释放 JDBC 资源。

```java
// Call a predefined utility class to create a new_department collection in SequoiaDB
SdbUtil.initCollection("sample", "new_department");
// Get Hive JDBC connection
Connection connection = HiveUtil.getConnection();
// Initialize Statement
Statement statement = null;
try {
    // Create Statement
    statement = connection.createStatement();
    // Drop the existing new_department mapping table
    String dropTable = "DROP TABLE IF EXISTS new_department";
    // Execute the SQL statement of drop mapping table
    statement.execute(dropTable);
    // Create new_department mapping table
    String mapping =
            "CREATE TABLE new_department(" +
                    "d_id int," +
                    "department string" +
                    ") USING com.sequoiadb.spark  " +
                    "OPTIONS( " +
                    "host 'sdbserver1:11810', " +
                    "collectionspace 'sample', " +
                    "collection 'new_department' " +
                    ")";
    // Execute the SQL statement of create mapping table
    statement.execute(mapping);
    // Bulk insert data from department table into new_department table
    String bulkInsert =
            "INSERT INTO new_department SELECT * FROM department";
    // Execute the SQL statement of bulk insert
    statement.executeUpdate(bulkInsert);
    // Call the query method to print the current new_department table result set
    getAll("new_department");
} catch (SQLException e) {
    e.printStackTrace();
} finally {
    // Release Hive JDBC source
    HiveUtil.releaseSource(null, statement, connection);
}
```

> **说明**
>
> ResultFormat 工具类为已经封装好的查询结果集打印类，用于美化结果集的打印；HiveUtil 工具类为已经封装好的 Hive JDBC 工具类，用于获取 JDBC 连接以及释放 JDBC 资源；SdbUtil 为 SequoiaDB 工具类，在当前实验中用于创建实验需要的集合。封装的工具类在此不做赘述。

3）将查询记录代码粘贴至 DataOperation 类 bulkInsert 方法的 TODO code 4 注释区间内。

![1738-440-TODO4](https://doc.shiyanlou.com/courses/1738/1207281/2f1bf19eb6e29e3a33eab8eeb9cd29a2-0)

代码块粘贴后效果如图所示（截图中 try 代码块已折叠）：

![1738-440-DONE4](https://doc.shiyanlou.com/courses/1738/1207281/5e16e1775c61d4a7e6b32ea1f261cc62-0)

4）右键点击 DataOperationMainTest 类，选择 Create/Edit DataOperationMainTest.main() 编辑主函数参数。

![1738-440-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/f6889e76389f71ce2371615a2ad45aee-0)

5）编辑主函数参数为 bulkinsert。

![1738-440-参数4](https://doc.shiyanlou.com/courses/1738/1207281/d359c135ffda056d516f70f63fae0115-0)

6）右键点击 DataOperationMainTest 类，选择 Run DataOperationMainTest .main() 运行程序。

![1738-440-运行1](https://doc.shiyanlou.com/courses/1738/1207281/54add0595ab5d3bb7caae53400ba9771-0)

7）查看运行结果。

![1738-440-结果4](https://doc.shiyanlou.com/courses/1738/1207281/6a165af580927d7b514e7773953e4c44-0)

可以看到 department 中的数据已全部插入到 new_derpartment 中。

## 总结

本课程介绍了如何通过 JDBC 对 Spark SQL 中的映射表进行查询、插入单条记录和批量插入记录到新表等数据操作，并简单介绍了 Spark 计算引擎在解析并处理 Spark SQL 提交的 SQL 时的工作机制（Catalyst Optimizer）。