---
show: step
version: 1.0 
---

## 课程介绍

本课程将介绍 Spark 中的 Job、Stage 和 Task 的概念，在实验中将通过 JDBC 在 Spark SQL 中进行关联与聚合操作，并通过 CREATE TABLE AS SELECT 的方式将查新创建为新表映射到 SequoiaDB 中的集合上。

#### 请点击右侧选择使用的实验环境

#### 部署架构

本课程中 SequoiaDB 巨杉数据库的集群拓扑结构为三分区单副本，其中包括：

* 1 个 SequoiaSQL-MySQL 数据库实例节点
* 1 个引擎协调节点
* 1 个编目节点
* 3 个数据节点

![1738-410-06](https://doc.shiyanlou.com/courses/1738/1207281/ff2754d609aba12340efeb27ce0645bb-0)

在当前实验的部署架构中，Spark 通过 MySQL 实例间接访问 SequoiaDB 存储集群，也可以通过 SequoiaDB 的 Spark 连接驱动直接访问底层的 SequoiaDB 存储集群。

详细了解 SequoiaDB 巨杉数据库系统架构：

* [SequoiaDB 系统架构](http://doc.sequoiadb.com/cn/sequoiadb-cat_id-1519649201-edition_id-0)

#### 实验环境

课程使用的系统环境为 Ubuntu 16.04.6 LTS 版本。SequoiaDB 数据库引擎以及 SequoiaSQL-MySQL 实例均为 3.4 版本。Spark 计算引擎为 2.4.3 版本。

## 打开项目

#### 打开 IDEA

1）打开 IDEA 代码开发工具。

![1738-410-07](https://doc.shiyanlou.com/courses/1738/1207281/72397a857808ab74f01b042f07ea0a27-0)

#### 打开 SCDD-Spark 项目

2）打开 Spark 实验的项目，在该项目中完成所有实验步骤。

![1738-410-08](https://doc.shiyanlou.com/courses/1738/1207281/5497d93bf19ee0442b4ae79b4cd8d39a-0)

#### Maven 依赖

实验环境中已经在 Maven 本地仓库添加了实验所需的依赖。当前实验使用到的 jar 包依赖以及依赖说明如下：

* hive-jdbc-1.2.1.spark.jar

  用于在 Java 程序中通过 JDBC 访问 Spark SQL

* spark-sequoiadb_2.11-3.2.4.jar

  用于在 Spark SQL 中创建 SequoiaDB 集合的映射

* mysql-connector-java-5.1.47.jar

  用于在 Java 程序中通过 JDBC 访问 MySQL 实例

* ibatis2-common-2.1.7.597.jar

  调用 ScriptRunner 执行 sql 文件写入 MySQL 实例，以此初始化 SequoiaDB 集合中的数据

pom.xml 文件位置：

![1738-410-pom文件位置](https://doc.shiyanlou.com/courses/1738/1207281/822fa966b397b80c9eabbf0472eb52c4-0)

当前实验中使用到的 Maven 依赖如下：

![1738-410-maven1](https://doc.shiyanlou.com/courses/1738/1207281/b0f437f18a0a276bbf404da17fef9a2c-0)

![1738-410-maven2](https://doc.shiyanlou.com/courses/1738/1207281/3d62b4c63338baa02ce4337f029ff166-0)

![1738-420-maven3](https://doc.shiyanlou.com/courses/1738/1207281/fd8e22c3304df6f328663a8b30b0c423-0)

![1738-420-maven4](https://doc.shiyanlou.com/courses/1738/1207281/e262833f73a5b167d89ba087e930642d-0)

> **说明**
>
> spark-sequoiadb_2.11-3.2.4.jar 可以在 [SequoiaDB 巨杉数据库下载中心](http://download.sequoiadb.com/cn/driver) 获取。

#### 打开当前实验的 Package

3）如图所示找到当前实验程序所在 Package，在该 Package 中完成后续实验步骤：

![1738-440-package](https://doc.shiyanlou.com/courses/1738/1207281/2f594d8742260180d5e4bbe5c2585c9e-0)

## 初始化数据并创建映射表

#### 概述

程序中对创建当前实验所需 SequoiaDB 集合，以及创建集合的Spark SQL 映射表操作进行了封装，在此只需执行程序准备后续关联查询、聚合查询实验步骤使用的实验数据。程序最终会将 employee、department 和 salary 表的数据打印至控制台。

#### 操作步骤

1）右键点击 AggregateMainTest 类，选择 Create/Edit AggregateMainTest.main() 编辑主函数参数。

![1738-450-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/9aa7b45d2eb67056f9dfbe671e18e0e4-0)

2）编辑主函数参数为 showmappingtables。

![1738-450-参数1](https://doc.shiyanlou.com/courses/1738/1207281/28190fed036a7ac99298109c42e4b049-0)

3）右键点击 AggregateMainTest 类，选择 Run AggregateMainTest .main() 运行程序。

![1738-450-运行1](https://doc.shiyanlou.com/courses/1738/1207281/f6bd5a3c8a7bc3ebacdacfbf1849819e-0)

4）查看运行结果。

![1738-450-结果1](https://doc.shiyanlou.com/courses/1738/1207281/e4267086f459e146c40982ec2621cd3c-0)

## 关联查询

#### 概述

![1738-410-05](https://doc.shiyanlou.com/courses/1738/1207281/e47c46e4b2ea1a76598667284f644dda-0)

在集群中，Spark 应用以独立的进程组的方式运行，并由主程序（driver program）中的 SparkContext  对象进行统一的调度。当需要在集群上运行时，SparkContext 会连接到几个不同类的 ClusterManager（集群管理器）上（Spark  自己的 Standalone/Mesos/YARN）, 集群管理器将给各个应用分配资源。连接成功后，Spark  会请求集群各个节点的 Executor（执行器），它是为应用执行计算和存储数据的进程的总称。之后，Spark 会将应用提供的代码（应用已经提交给  SparkContext 的 JAR 或 Python 文件）交给 Executor。最后，由 SparkContext 发送Tasks 提供给 Executor 执行（多线程）。

程序提交给 Spark 的计算请求可以抽象为一个 Job，Spark 的 Driver Manager 会将其转化成为一个 DAG（有向无环图），这里可以将其简单理解为一个逻辑执行计划，DAG 是由多个任务单元 Stage 组成的，Stage 将 DAG 分为了多个执行阶段，最终 Stage 会被拆解成一个个 Task 根据物理执行计划传达给相应的 Executor 执行。 Job、Stage 和 Task，三者之间的关系如下图所示：

![1738-450-01](https://doc.shiyanlou.com/courses/1738/1207281/0f064f9b7bba2fe40b2eedd04d5b8c9b-0)

以 Spark SQL 中提交关联聚合查询为例，查询会作为一个 Job，Spark 的 Driver Manager 会将查询转化成为 DAG，查询的子查询、聚合等过程对应 DAG 的 Stage 任务单元，根据查询的执行计划生成的 Task 会被分配到相应的 Executor 中被执行（当前实验环境相当于在 SequoiaDB 存储集群的不同数据分区中查询）。

当前实验步骤将简单演示对 Spark 中的 department 和 salary 映射表进行关联查询，以获取各个部门中所有职位的薪资情况。

#### 操作步骤

1）打开 Aggregate 类。

![1738-450-打开Aggregate](https://doc.shiyanlou.com/courses/1738/1207281/430358ca9448dd2f88f3dac462692cc3-0)

2）复制关联查询代码。程序中将调用封装好的类获取 JDBC 连接，使用 JDBC 连接创建 Statement 执行关联查询 SQL 语句，并将查询结果集打印至控制台。所有数据操作执行完毕后将释放 JDBC 资源。

```java
// Get Hive JDBC connection
Connection connection = HiveUtil.getConnection();
// Initialize Statement
Statement statement = null;
// Initialize ResultSet
ResultSet resultSet = null;
try {
    // Create Statement
    statement = connection.createStatement();
    // joinQuery
    String joinQuery =
            "SELECT s.id,d.department,s.position,s.salary " +
                    "FROM " +
                    "department d,salary s " +
                    "WHERE s.department = d.d_id";
    // Execute the SQL statement of joinQuery
    resultSet = statement.executeQuery(joinQuery);
    // Call the predefined result set to beautify the utility class and print the result set
    ResultFormat.printResultSet(resultSet);
} catch (SQLException e) {
    e.printStackTrace();
} finally {
    // Release Hive JDBC source
    HiveUtil.releaseSource(resultSet, statement, connection);
}
```

> **说明**
>
> ResultFormat 工具类为已经封装好的查询结果集打印类，用于美化结果集的打印；HiveUtil 工具类为已经封装好的 Hive JDBC 工具类，用于获取 JDBC 连接以及释放 JDBC 资源。封装的工具类在此不做赘述。

3）将查询记录代码粘贴至 Aggregate 类 joinQuery 方法的 TODO code 1 注释区间内。

![1738-450-TODO1](https://doc.shiyanlou.com/courses/1738/1207281/a09c5e5f020b27716ab4ad796f52c741-0)

代码块粘贴后如图所示（为展示效果已将 try 代码块折叠）：

![1738-450-DONE1](https://doc.shiyanlou.com/courses/1738/1207281/bdb81d08ab808f50c01aa8f36f717d08-0)

4）右键点击 AggregateMainTest 类，选择 Create/Edit AggregateMainTest.main() 编辑主函数参数。

![1738-450-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/9aa7b45d2eb67056f9dfbe671e18e0e4-0)

5）编辑主函数参数为 join。

![1738-450-参数2](https://doc.shiyanlou.com/courses/1738/1207281/4343b140f511d1b38df310226bc78278-0)

6）右键点击 AggregateMainTest 类，选择 Run AggregateMainTest .main() 运行程序。

![1738-450-运行1](https://doc.shiyanlou.com/courses/1738/1207281/f6bd5a3c8a7bc3ebacdacfbf1849819e-0)

7）查看运行结果。

![1738-450-结果2](https://doc.shiyanlou.com/courses/1738/1207281/0375bef281c18d98a62a9f729373d861-0)

## 聚合查询

#### 概述

当前实验步骤中将通过 CREATE TABLE AS SELECT 的方式在 Spark SQL 中创建映射表写入聚合查询结果，以此来将聚合查询的结果生成为统计表。这种方式创建映射表会在 SequoiaDB 中自动创建指定的映射集合。集合查询内容为计算各个部门的人均工资并统计人数。

#### 操作步骤

1）打开 Aggregate 类。

![1738-450-打开Aggregate](https://doc.shiyanlou.com/courses/1738/1207281/430358ca9448dd2f88f3dac462692cc3-0)

2）复制聚合查询生成统计表代码。程序中将调用封装好的类获取 JDBC 连接，使用 JDBC 连接创建 Statement 执行聚合查询建表 SQL 语句，并将查询结果集打印至控制台。所有数据操作执行完毕后将释放 JDBC 资源。

```java
// Get Hive JDBC connection
Connection connection = HiveUtil.getConnection();
// Initialize Statement
Statement statement = null;
// Initialize ResultSet
ResultSet resultSet = null;
try {
    // Create Statement
    statement = connection.createStatement();
    // Drop the existing table
    String dropStatistic = "DROP TABLE IF EXISTS statistic";
    // Execute the SQL statement of drop table
    statement.execute(dropStatistic);
    // Generate statistical tables of aggregate query results in SparkSQL using CTAS
    String aggregate =
            "CREATE TABLE statistic USING com.sequoiadb.spark  " +
                    "OPTIONS( " +
                    "host 'sdbserver1:11810', " +
                    "collectionspace 'sample', " +
                    "collection 'statistic' " +
                    ")AS( " +
                    "SELECT d.d_id,d.department,avg_table.salary_avg,avg_table.employee_num " +
                    "FROM( " +
                    "SELECT s.department as department_id,avg(s.salary) AS salary_avg,count(1) AS employee_num " +
                    "FROM employee e,salary s " +
                    "WHERE e.position = s.id " +
                    "GROUP BY s.department " +
                    ")AS avg_table " +
                    "LEFT JOIN department d " +
                    "ON avg_table.department_id = d.d_id)";
    // Execute the SQL statement to generate statistics table
    statement.execute(aggregate);
    // Query statistic
    String queryStatistic = "SELECT * FROM statistic";
    // Execute the SQL statement of query statistic table
    resultSet = statement.executeQuery(queryStatistic);
    // Call the predefined result set to beautify the utility class and print the result set
    ResultFormat.printResultSet(resultSet);
} catch (SQLException e) {
    e.printStackTrace();
} finally {
    // Release Hive JDBC source
    HiveUtil.releaseSource(resultSet, statement, connection);
}
```

> **说明**
>
> ResultFormat 工具类为已经封装好的查询结果集打印类，用于美化结果集的打印；HiveUtil 工具类为已经封装好的 Hive JDBC 工具类，用于获取 JDBC 连接以及释放 JDBC 资源。封装的工具类在此不做赘述。

3）将查询记录代码粘贴至 Aggregate 类 aggregateQuery 方法的 TODO code 2 注释区间内。

![1738-450-TODO2](https://doc.shiyanlou.com/courses/1738/1207281/169d7a1d959a438b896057d6ccebcd4a-0)

代码块粘贴后如图所示（为展示效果已将 try 代码块折叠）：

![1738-450-DONE2](https://doc.shiyanlou.com/courses/1738/1207281/ceafb8239eff166d0a0615c7d1213d01-0)

4）右键点击 AggregateMainTest 类，选择 Create/Edit AggregateMainTest.main() 编辑主函数参数。

![1738-450-编辑1](https://doc.shiyanlou.com/courses/1738/1207281/9aa7b45d2eb67056f9dfbe671e18e0e4-0)

5）编辑主函数参数为 aggregate。

![1738-450-参数3](https://doc.shiyanlou.com/courses/1738/1207281/fd0715493b8b5e270ac0b0b5b7588a18-0)

6）右键点击 AggregateMainTest 类，选择 Run AggregateMainTest .main() 运行程序。

![1738-450-运行1](https://doc.shiyanlou.com/courses/1738/1207281/f6bd5a3c8a7bc3ebacdacfbf1849819e-0)

7）查看运行结果。

![1738-450-结果3](https://doc.shiyanlou.com/courses/1738/1207281/7beb64c869408cc532c3e81620bee45e-0)


## 总结

本课程简要介绍了 Spark 任务的 Job、Stage 和 Task 等概念。在实验中通过 JDBC 对 Spark SQL 中的映射表进行关联与聚合操作，并介绍了如何通过 CREATE TABLE AS SELECT 的方式将查询结果创建为新表映射 SequoiaDB 的集合。

